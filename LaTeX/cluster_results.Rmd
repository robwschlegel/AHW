---
title: "Cluster Results"
# author: "Robert Schlegel"
# date: "30 March 2017"
output: pdf_document
bibliography: AHW.bib
csl: elsevier-harvard.csl
---

# Concepts that need to be investigated
Much has been done thus far in regards to the research, and a clear picture of what is further required now exists. The work with SOMs is likely sound, but does require that a few more variables be tested. Furthermore, it is not yet certain that SOMs will be the best clustering technique. To that end, K-means clustering and hierarchical clustering have also been identified as alternative techniques. This now gives rise to the possibility that two papers could emerge from this work. One on the resultant clustering of synoptic air-sea states during coastal MHWs, and another paper that discusses the strengths of these various clustering techniques.

## The metrics for each MHW in each cluster
This requires that once the different events have been clustered, regardless of the technique used, or the variables controlled for within (see below), a summary of the event metrics must also be provided. These then will allow for the second more meaningful round of the interpretation of the results.

![The results of a SOM clustering of the syoptic air-sea anaomaly data during coastal MHWs. The clusters shown here correspond to the following table and figure.](~/AHW/graph/som/anom_9.pdf)

```{r, warning=FALSE, message=FALSE, echo=FALSE, results='asis'}
# Load SOM results from anomaly data prepared by "proc/results.som.R"
load("results/node_all_anom.Rdata")
# Run the metric summary function on these data
source("~/AHW/func/som.func.R")
node_all_anom_table <- node.summary.metrics(node_all_anom)
## This produces text that LaTeX likes
# library(xtable)
# xtable(node_all_anom_table, caption = "The possible metrics that may be of interest for summarising the events clustered into each node.")
## This produces a better default for RMarkdown, but doesn't produce captions without a bit of faffing about
## See: http://stackoverflow.com/questions/19997242/simple-manual-rmarkdown-tables-that-look-good-in-html-pdf-and-docx
library(pander)
pander(node_all_anom_table, caption = "The possible metrics that may be of interest for summarising the events clustered into each node. Node numbers given here correspond to Figure 1 and 2.")
```
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Lolliplots showing start date and cummulative intensity of each event by each node, as seen in Figure 1."}
# Visualise the start dates and int_cum of the events via lolliplots
library(RmarineHeatWaves)
library(ggplot2)
node_all_anom_2 <- merge(node_all_anom, event_list, by = c("event", "site", "season", "event_no"))
ggplot(data = node_all_anom_2, aes(x = date_start, y = int_cum)) +
  geom_lolli() +
  facet_wrap(~node) +
  labs(x = "date", y = "cummulative intensity (Cxdays)")
```

Figure 2 most clearly demonstrates that the SOM nodes in Figure 1 generally consist of either several events that occurred at the same time, or a blend of many disparate events. The nodes that contain more events are therefore much more 'neutral', meaning there is very little visible by way of air-sea anomalies. This is as I had feared and is currently my largest criticism of this technique. If one were to add more SOM nodes (a bad idea given that there are only 95 data vectors to be clustered), the new nodes will only allow some of the events within the larger clusters to break away and form their own node. The complexity of the air-sea states is such that any consistent patterns that may occur are obfuscated by everything else happening in the study area. Therefore it is necessary to reduce the dimensions of the input data by drawing a more narrow box around the study area to investigate the effect this may have. But first, the BRAN data need to be appropriately rounded down to a resolution of 0.5 degrees in order to match the ERA-Interim data.

## Effect of pixel resolution on clustering
The more dimensions/ variables one introduces to a cluster analysis, the more stress will exist in the results. As large stress values are generally considered to be a negative result in clustering, it is best to attempt to reduce it where possible. One way of doing that for this research is by reducing the pixel resolution of the reanalysis products. There are two reasons that this cannot simply be done out of hand. The first is that the reduction in resolution may affect the clustering of the events. So this must be documented. The other problem this presents is that the reduction of pixel resolution would require that any results produced be shown at this same reduced resolution. And because the goal is to show meso-scale forcing on the coast, higher pixel resolutions would be preferable. Regardless, the ERA-Interim data are at a resolution of 0.5 degrees, which requires that the BRAN data be reduced to this same resolution for appropriate cluster comparison. Beyond this initial required reduction in resolution, the question then is what effect does the further rounding of the data produce? Here we look at three resolutions: 0.5, 1.0 and 2.0 degree lon/ lat.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
## Here the data are transformed and saved and re-loaded for convenience
# "all_anom.Rdata" was prepared by "proc/results.som.R"
# load("results/all_anom.Rdata")
# system.time(all_anom_0.5 <- synoptic.round(all_anom, 0.5)) # 115 seconds
# save(all_anom_0.5, file = "results/all_anom_0.5.Rdata")
# load("results/all_anom_0.5.Rdata")
# system.time(all_anom_1.0 <- synoptic.round(all_anom, 1.0)) # 134 seconds
# save(all_anom_1.0, file = "results/all_anom_1.0.Rdata")
# load("results/all_anom_1.0.Rdata")
# system.time(all_anom_2.0 <- synoptic.round(all_anom, 2.0)) # 126 seconds
# save(all_anom_2.0, file = "results/all_anom_2.0.Rdata")
# load("results/all_anom_2.0.Rdata")

## After loading the clustering is performed
## Here SOMs are used
# system.time(som_all_anom <- som.model(all_anom)) # 123 seconds
# system.time(som_all_anom_0.5 <- som.model(all_anom_0.5)) # 4 seconds
# system.time(som_all_anom_1.0 <- som.model(all_anom_1.0)) # 1 seconds
# system.time(som_all_anom_2.0 <- som.model(all_anom_2.0)) # 1 seconds

## Create node indeces and save
# node_all_anom <- event.node(all_anom, som_all_anom)
# save(node_all_anom, file = "results/node_all_anom.Rdata")
# node_all_anom_0.5 <- event.node(all_anom_0.5, som_all_anom_0.5)
# save(node_all_anom_0.5, file = "results/node_all_anom_0.5.Rdata")
# node_all_anom_1.0 <- event.node(all_anom_1.0, som_all_anom_1.0)
# save(node_all_anom_1.0, file = "results/node_all_anom_1.0.Rdata")
# node_all_anom_2.0 <- event.node(all_anom_2.0, som_all_anom_2.0)
# save(node_all_anom_2.0, file = "results/node_all_anom_2.0.Rdata")

## Load node indeces and calculate differences
load("results/node_all_anom.Rdata")
load("results/node_all_anom_0.5.Rdata")
load("results/node_all_anom_1.0.Rdata")
load("results/node_all_anom_2.0.Rdata")

# node_res <- node_all_anom
node.count <- function(node_res, resolution){
  df_1 <- unique(node_res[,2:3])
  df_1 <- df_1[order(df_1$node),]
  rownames(df_1) <- df_1$node
  df_1 <- data.frame(df_1[,2])
  colnames(df_1) <- resolution
  return(df_1)
}

node_diff <- data.frame(node_all = node.count(node_all_anom, "all"),
                       node_0.5 = node.count(node_all_anom_0.5, "0.5"),
                       node_1.0 = node.count(node_all_anom_1.0, "1.0"),
                       node_2.0 = node.count(node_all_anom_2.0, "2.0"))
colnames(node_diff) <- c("res_all", "res_0.5", "res_1.0", "res_2.0")
pander(node_diff, caption = "Table showing the number of events clustered into which of the 9 SOM nodes. The different columns show the effect that reducing the resolution of the data has on the clustering.")
```

The results table generated from the clustering of events into different nodes shown above is not very informative because it is known that the SOM algorithm always reshuffles these data into different nodes. Due to the very high dimensions of the data, the algorithm is not able to find a single best answer. Therefore, it is more informative to further order each column from highest to lowest so as to see if the general clustering of events is similar.
```{r, warning=FALSE, message=FALSE, echo=FALSE}
node_diff_2 <- apply(node_diff, 2, sort)
pander(node_diff_2, caption = "Table showing the number of events within a node scored in descending order. The different columns show the effect that reducing the resolution of the data has on the clustering.")
```

When the data are reshuffled into descending order based on the number of events clustered into each node we see that the results are much more similar than they first appeared. The next step in this portion of the analysis is not the creation of figures, but rather cclustering events that are generally clustered together. This then would allow for the comparison to be scaled up so it could be replicated 1,000 times to be more thorough.


## Comparing large replication sets
Because the SOM nodes are shuffled during each run, comparing the results directly becomes difficult. We can however iterate the SOM analysis many times and save a vector of results for each event showing into which node it wa cast for each run. This then creates a unique information vector for each event that can then be used to further cluster the events into 'mean' clusters. This helps to address the issue of SOM node 'drift'. The following two tables show the results of a SOM run on the data 10 times.
```{r, warning=FALSE, message=FALSE, echo=FALSE}
# Load 0.5 res data
load("results/all_anom_0.5.Rdata")

## Run the analysis once
# system.time(som_all_anom_1r <- som.model(all_anom_0.5)) # 4 seconds
# node_all_anom_1r <- event.node(all_anom_0.5, som_all_anom_1r)
# save(node_all_anom_1r, file = "results/node_all_anom_1r.Rdata")
load("results/node_all_anom_1r.Rdata")
node_all_anom_1r <- node_all_anom_1r[,1:3]
# colnames(node_all_anom_1r) <- c("event", "node_1", "count_1")

## Run the analysis 10 times
# node_all_anom_10r <- node_all_anom_1r
# for(i in 2:10){
#   som_all_anom_10r <- som.model(all_anom_0.5)
#   df <- event.node(all_anom_0.5, som_all_anom_10r)
#   node_all_anom_10r <- cbind(node_all_anom_10r, df[,2:3])
# }
# save(node_all_anom_10r, file = "results/node_all_anom_10r.Rdata")
load("results/node_all_anom_10r.Rdata")

node_diff_10 <- node.count(node_all_anom_10r, "1r")
for(i in 2:10){
  j <- i*2
  res <- node.count(node_all_anom_10r[,c(1,j,j+1)], paste0(i,"r"))
  node_diff_10 <- cbind(node_diff_10, res)
}
pander(node_diff_10, caption = "Table showing the number of events within the same node over 10 runs on the same data. Note how variable the node assignments may be.")
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
# Order the 10 SOM run in descending order
node_diff_10_2 <- apply(node_diff_10, 2, sort)
pander(node_diff_10_2, caption = "Table showing the number of events within a node scored in descending order. The different columns show each run. Note that the results are generally consistent, but not completely.")
```

Now let's up the anti a bit and run the SOM 100 times. Because the resultant data frame will become unwieldly, we will create means across the number of clustered events as coerced into descending order. These then will be compared to the mean for the 1 run and 10 run data frames.
```{r, warning=FALSE, message=FALSE, echo=FALSE}
## Run the analysis 100 times
# node_all_anom_100r <- node_all_anom_10r
# for(i in 11:100){
  # som_all_anom_100r <- som.model(all_anom_0.5)
  # df <- event.node(all_anom_0.5, som_all_anom_100r)
  # node_all_anom_100r <- cbind(node_all_anom_100r, df[,2:3])
# }
# save(node_all_anom_100r, file = "results/node_all_anom_100r.Rdata")
load("results/node_all_anom_100r.Rdata")

# Compute counts per node
node_diff_100 <- node.count(node_all_anom_1r, "1r")
for(i in 2:100){
  j <- i*2
  res <- node.count(node_all_anom_100r[,c(1,j,j+1)], paste0(i,"r"))
  node_diff_100 <- cbind(node_diff_100, res)
}

# Order
node_diff_100_2 <- apply(node_diff_100, 2, sort)

# Create means from each dataframe
node_diff_1_mean <- data.frame(node_diff_100_2[,1])
colnames(node_diff_1_mean) <- "1r"
node_diff_10_mean <- data.frame(round(apply(node_diff_100_2[,1:10], 1, mean),0))
colnames(node_diff_10_mean) <- "10r"
node_diff_100_mean <- data.frame(round(apply(node_diff_100_2, 1, mean),0))
colnames(node_diff_100_mean) <- "100r"
node_diff_mean <- cbind(node_diff_1_mean, node_diff_10_mean, node_diff_100_mean)

# Table
pander(node_diff_mean, caption = "Table showing the number of events within a node scored in descending order. The different columns show the mean of 1, 10, and 100 SOM runs. Note that the results are remarkably similar.")
```

As we may see from the table above, the 'wobble' present in the SOM analysis is not great. I considered running the analysis 1,000 times but this would take several hours on my computer and I think that the consistency shown between 1, 10, and 100 runs is sufficient to put this question to rest.


## Effect of lat/ lon extent on clustering
With more traditional cluster analyses, the values being compared would have far fewer dimensions. In this regard one would endeavour to only include variables that seem relevant to the question being asked. For example, if clustering different rock pools by the species found within them, one would likely create better results by not including any anomalous species found in the results (like a cow fish). In regards to this work, it is best to include only the pixels that are likely relevant to the meso-scale features that may be impacting the coast. More specifically, cutting out the Agulhas retroflection above the Southern Ocean will prevent any behaviour there from affecting the clustering of events that are occurring along the coastline of South Africa.

That all being said, it may indeed be relavent, at least in the sense of potential teleconnections, to include the Agulhas retroflections over the Southern Ocean. Therefore, I have decided to trim the total study area by 1 degree on the East, South and West extents, rather than just the South extent. The North border of the study are is left unchanged as this would begin to remove some of the coastal stations. The effect of trimming the study area 1 degree at a time is presented below. A SOM is run on each trimmed data frame 10 times to produce smoother results. The inquiry into the SOM 'drift' above shows that 10 iterations return comparable results to 100 iterations, so 10 are used here in the interest of speed. A total of 5 degrees are iteratively trimmed.

```{r, warning=FALSE, message=FALSE, echo=FALSE}
## Trim the data
# all_anom_0.5_trim_1 <- synoptic.trim(all_anom_0.5, 1)
# save(all_anom_0.5_trim_1, file = "results/all_anom_0.5_trim_1.Rdata")
load("results/all_anom_0.5_trim_1.Rdata")
# all_anom_0.5_trim_2 <- synoptic.trim(all_anom_0.5, 2)
# save(all_anom_0.5_trim_2, file = "results/all_anom_0.5_trim_2.Rdata")
load("results/all_anom_0.5_trim_2.Rdata")
# all_anom_0.5_trim_3 <- synoptic.trim(all_anom_0.5, 3)
# save(all_anom_0.5_trim_3, file = "results/all_anom_0.5_trim_3.Rdata")
load("results/all_anom_0.5_trim_3.Rdata")
# all_anom_0.5_trim_4 <- synoptic.trim(all_anom_0.5, 4)
# save(all_anom_0.5_trim_4, file = "results/all_anom_0.5_trim_4.Rdata")
load("results/all_anom_0.5_trim_4.Rdata")
# all_anom_0.5_trim_5 <- synoptic.trim(all_anom_0.5, 5)
# save(all_anom_0.5_trim_5, file = "results/all_anom_0.5_trim_5.Rdata")
load("results/all_anom_0.5_trim_5.Rdata")
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
# Create a function that runs 10 SOM analyses and produces mean results
# df <- all_anom_0.5_trim_5
som.10.mean <- function(df, trim){
  # Run the analysis once to set the stage
  som_res <- som.model(df)
  node_res <- event.node(df, som_res)
  node_res <- node_res[,1:3]
for(i in 2:10){
  som_res <- som.model(df)
  df_1 <- event.node(df, som_res)
  node_res <- cbind(node_res, df_1[,2:3])
}
# Compute counts per node
node_res_10 <- node.count(node_res, "1r")
for(i in 2:10){
  j <- i*2
  res <- node.count(node_res[,c(1,j,j+1)], paste0(i,"r"))
  node_res_10 <- cbind(node_res_10, res)
}
# Order
node_res_10 <- apply(node_res_10, 2, sort)
# Create means from each dataframe
node_res_10_mean <- data.frame(round(apply(node_res_10, 1, mean),0))
colnames(node_res_10_mean) <- paste0("trim_",trim)
return(node_res_10_mean)
}
```
```{r, warning=FALSE, message=FALSE, echo=FALSE}
# Compute the SOM results for the different trims
# trim_0_nodes <- data.frame(node_diff_mean[,2])
# colnames(trim_0_nodes) <- "trim_0"
# trim_1_nodes <- som.10.mean(all_anom_0.5_trim_1, 1)
# trim_2_nodes <- som.10.mean(all_anom_0.5_trim_2, 2)
# trim_3_nodes <- som.10.mean(all_anom_0.5_trim_3, 3)
# trim_4_nodes <- som.10.mean(all_anom_0.5_trim_4, 4)
# trim_5_nodes <- som.10.mean(all_anom_0.5_trim_5, 5)

# Combine and save
# trim_all_nodes <- cbind(trim_0_nodes, trim_1_nodes, trim_2_nodes,
#                         trim_3_nodes, trim_4_nodes, trim_5_nodes)
# save(trim_all_nodes, file = "results/trim_all_nodes.Rdata")
load("results/trim_all_nodes.Rdata")
# Table
pander(trim_all_nodes, caption = "Table showing the number of events within a node scored in descending order. The different columns show the mean of 10 SOM runs on differing amounts of spatial reduction in the extent of the study area. Note that the results remain similar, but there appears to be break in the results once 3 degrees of lon & lat have been removed from the East, South, and West edges of the study area.")
```

The table above shows that removing portions of the study area does have an effect on the SOM clustering of the data. Leaving the study area 'as is' is shown in the column labeled 'trim_0'. These are the results shown in the previous tables and figures. Removing one degree of lat/ lon appears to cluster the events more into the one large cluster, with fewer events in the smaller clusters. Removing 2 degrees of lat/ lon reverses this trend. Trimming 3 to 5 degrees of lat/ lon then all appears to produce very similar results. The difference between these different extents is not large, but I think it does show that the edges of the study area are having an effect on the clustering. And that reducing the area does allow for a more even clustering of the events into nodes. It may be best to use a study area with 3 degrees trimmed from the East, South, and West edges. But for now I shall continue to use the full study area.


## Effect of running air and sea variables separately
It may be that air and sea values work in tandem with one another to force MHWs, but it is more likely that they do not. Except for perhaps VERY extreme situations (e.g. once per decade). Therefore it is necessary to run all clustering techniques on air-sea values combined, as well as separately, in order to quantify the potential effect they have on clustering.

```{r}

```


## Do normal days cluster
The idea here is to include the 366 daily synoptic climatology values in with the synoptic MHW values to see if they cluster differently.

```{r}

```

# Clustering techniques that need to be investigated

# Hierarchical clustering
HCA differs from the other two techniques outlined below in that it does not cluster the data simultaneously, based on the least stress that can be found between data vectors. Rather it iteratively divides (or combines) data vectors as the algorithm moves down (or up) a classification tree. Always looking for the point at which clusters of data may be split (or combined). This method may benefit this research 

## Dendrograms
```{r}

```


# K-means clustering
The simplest method of clustering, and for that reason still one of the best. This is a basic algorithm that takes all data vectors and positions them in a 2D space. It then picks K points and sees, given the best possible fit of all dimensions being used, which data vectors are closest to which of the K points. This process is then repeated x number of times until a best fit is found. The data vectors are classified into the cluster centroid to which they are closest.

## Ordiplots
```{r}

```


# SOMs
The originally proposed technique and perhaps, once this dust settles, the reigning champion. The SOM technique is apart from the previous two methods in that it accounts for the gradient that exists between the nodes it clusters the given data into. Meaning that the positions of the nodes in 2D space is relevant, unlike HCA and K-means.

## SOM nodes
```{r}

```

<!-- # MDS -->
<!-- Multi-dimensional scaling provides another possible layer of interpretation of these data. By highlighting which pixels on the map belong to which meso-scale properties (e.g. Agulhas, Benguela, Agulhas retroflection) it is then possible to overlay the effect of these pixels, and therefore meso-scale features, on top of the ordiplots generated by MDS. -->

<!-- ## Ordiplots -->

# References